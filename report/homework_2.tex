\documentclass{article} % For LaTeX2e
\usepackage[utf8]{inputenc}
\usepackage{cos424,times}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{enumitem}
\usepackage{hyperref}

\graphicspath{{./figures/}} 

\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\I}{\mathbb{I}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{Classification of Review Sentiments}


\author{
Vivek Kumar\\
Civil and Environmental Engineering\\
Princeton University\\
\texttt{vivekk@princeton.edu} \\
\And
Victor Charpentier\\
Civil and Environmental Engineering\\
Princeton University\\
\texttt{@princeton.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Applying machine learning techniques for social good is one of the bright outcomes of the technological revolution of the past years. In homework 2 we have performed a prediction and analysis of 6 key outcomes at age 15 of the fragile family challenge. The methods implemented depend on the type of outcome. For each type of outcome over 6 prediction methods are tested. The performance of regressions and classifiers is assessed with xx and receiver operating curves. Overall, the random forest regression methods has the best results for continuous variables and XX has the best result for binary outcomes. Finally, the confidence intervals of the predictors are computed to quantify the quality of the prediction.

\end{abstract}

\section{Introduction}
The fragile family challenge (FFC) is a Princeton University-led initiative opening the data collected in the long term Fragile Family and Child Wellbeing Study (FFCWS) to a variety of data scientists. The study follows an ensemble of nearly 4700 families with children born in 20 cities across the U.S. between 1998 and 2000. The study has an over-representation of non-marital birth compared to national average. The goal of the study is to provide information on the family factors that influence the life of the child  \cite{reichman2001fragile}. The FFC was officially closed in May 2017, and its winner announced in the fall of 2017. 
With an increased understanding of the data gained from the challenge, we aim to apply regressions and classifications to the dataset of the FFC to predict the 6 key outcomes. Regressions and classification constitute the backbone of supervised learning. However beyond the purely mathematical aspect of the challenge, feature selection and dataset preparation are some of the most important aspects of a real machine learning project. So important that some describe the refinement of feature or feature engineering as a ''\textit{black art}'' \cite{domingos2012few}. With respect to this mystical reference we have worked to prepare the training data for prediction. The continuous outcomes (GPA, grit and material hardship) have been predicted using Random Forest Regressor (RF), Ada Boost Regressor (AB), lasso regression (LRCV), ridge regression (RRCV), elastic net (EN), extra trees regressor (ET) and multi-layer perceptron regressor (MLP). The binary outcomes (eviction, job training and layoff) were predicted with quadratic discriminant analysis (QLA), logistic regression (LR), random forest classifier (RFC), ada boost classifier (AB),  multi-layer perceptron classifier (MLP) and extra trees classifier (EF).


\section{Description of the Data}
In the domains that generate a lot of data, social sciences are maybe one of the least explored by data scientist. Social sciences excel in causal inference, they have handled data from observations for a long time. Data science on the other end has experience with large dataset but is not focused so much on causal inference \cite{grimmer2015we}. The hope of applying machine learning techniques to the FFCWS data-set is to combine both sciences for the understanding of fragile families outcomes. Improving the methods to treat large data-sets is vital for social sciences and the potential outcomes could be transformative for society.

Inequalities in the U.S. are threatening the model of the American Dream. Some voices from the highest economic international agencies are calling for a new American Dream, "\textit{based on equality and sustainable growth}" \cite{wef2017}. In this context, understanding what factors positively influence children growing up in some of the most difficult family environments may be key to creating policies more able to grant equal opportunities for all. The study follows an ensemble of nearly 4700 families with children born in 20 cities across the U.S. between 1998 and 2000. The study has an over-representation of non-marital birth compared to national average. The goal of the study is to provide information on the family factors that influence the life of the child  \cite{reichman2001fragile}.


\section{Overview of the methods and fit to reference data}
\subsection{Data wrangling}
The input data for the homework consists of two files. The true values of the outcomes for 2121 families and the data set of over 15 000 features for all the 4242 families. As identified in the introduction, feature selection and engineering can be seen as an "art". Therefore we spent some time on making sure the quality of data that was going to be used for prediction was satisfying. 
The preparation of the true outcomes is a two step process. In the first step, the families that have outcomes values equal to 6 NAs are dropped. The goal of that step was to reduce the skew of the training data toward the average of each outcome. 655 families were removed from the training data as a result. This is a big number but we assume that the missing information of the outcomes carries some meaning. The training outcomes still contain some NA values. However for these cases some of the other outcomes are available. In the second step the dataset is cleaned. First the constant variables provided in the file {\fontfamily{qcr}\selectfont constantVariables.txt} are removed. Some survey weights of the type "\texttt{q5natwt\char`_xxxx}", "\texttt{q5natwtx\char`_xxxx}" and "\texttt{q5citywt\char`_xxxx}" were placed in the list of constant variables. We decided to consider the information embedded in missing data. All values of missing data less than -5 are considered empty from meaning and set to 0. The missing data from -1 to -4 is maintained. Finally the answers of the type "if not in the list specify:" (e.g. \texttt{d3c5a\char`_14ot}) are coded to 1 if there is a string in the cell and to 0 if the cell is empty.
\subsection{Prediction}
\section{Detailed presentation of the regression forest}
The random forest predictor used for regression, also called regression forest (RF), is based on the stand alone model of decision trees. Decision trees are prone to over fitting and sensitive to missing data. Therefore, a forest stems (pun) from the aggregate of many weak models to produce a better overall prediction. As opposed to a classification forest, a regression forest provides prediction of continuous variables. The input data seen on Figure \ref{fig:rf} is continuous. Therefore the leaf nodes predict real values (as opposed to classes). The data-set is split based on homogeneity of data (with the standard deviation). This leads to subsets of the data contains similar values of the data to predict (cf. Figure \ref{fig:rf}(a)). The similarity is quantified by entropy, a measure of predictability. The form of the predictor can vary once the tree is fully grown. Several types of predictors can be used \cite{criminisi2011decision}. They can be constant, linear, polynomial or probabilistic-linear among others  \ref{fig:RFpredictors}.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{rf}
	\caption{Detail of a regression forest - (a) the continuous data is approximated by a piecewise continuous function. The subdivision of the x interval stops when the entropy or SSE has reached a threshold value - (b) an example of regression tree from 	\cite{criminisi2011decision}, the leaves of the tree determine values of continuous outputs for subdomains of x}
	\label{fig:rf}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{rfpredictors}
	\caption{Example of three predictor models (a) constant - (b) linear and (c) polynomial}
	\label{fig:RFpredictors}
\end{figure}

In the case of the constant predictor, the value of the predictor is given by the minimization of the sum of squared error (SSE) as given by Equation \ref{sse_eqn} for the subdomain $D_k$. 

\begin{equation} \label{sse_eqn}
\hat{y}_k = \argmin_y \sum_{i \in D_k} (y-y_i)^2
\end{equation}

Two parameters appears as main control nobs of the method: the number of regression trees in the ensemble and the depth of those trees. Each of those parameter has a distinct effect on the prediction. The depth of the trees controls the closeness of the fit. A ensemble of trees of depth 1 will correspond to a linear regression since each tree corresponds to its root node. In comparison, an ensemble of very deep trees will risk being over-fitted. The number of trees in the forest influences the smoothness of the prediction, the more trees the smoother the direction of the prediction. 
Finally regression forest is a preferred method due to its speed, efficiency, ability to cope with missing data and flexibility of use. 
\section{Presentation of results}

\begin{enumerate}
	\item Build a model for predicting 1-type of variable well.
	\item GPA, Grit, Material Hardship
	\begin{enumerate}
		\item Hypothesis : Positive Environment + Lack of Negative leads to positive GPA + ..
		\item Hypothesis : Positive Environment + Plus some negativity + ..
		\item Hypothesis : Negative Environment + Lack of parent + ..
	\end{enumerate}
	\item Drop NA values for training: only in case of NA in all.
\end{enumerate}
\section{Discussion of the results}
\section{Conclusions}



\bibliography{ref}
\bibliographystyle{plain}
\end{document}
